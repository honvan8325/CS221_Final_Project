{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14288948,"sourceType":"datasetVersion","datasetId":9120614}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gliner","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T01:30:18.900936Z","iopub.execute_input":"2025-12-26T01:30:18.901235Z","iopub.status.idle":"2025-12-26T01:30:35.090702Z","shell.execute_reply.started":"2025-12-26T01:30:18.901208Z","shell.execute_reply":"2025-12-26T01:30:35.089832Z"}},"outputs":[{"name":"stdout","text":"Collecting gliner\n  Downloading gliner-0.2.24-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from gliner) (2.8.0+cu126)\nCollecting transformers>=4.57.3 (from gliner)\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub>=0.21.4 in /usr/local/lib/python3.12/dist-packages (from gliner) (0.36.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gliner) (4.67.1)\nCollecting onnxruntime (from gliner)\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from gliner) (0.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.4->gliner) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->gliner) (3.4.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.57.3->gliner) (2.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.57.3->gliner) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.57.3->gliner) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.57.3->gliner) (0.6.2)\nCollecting coloredlogs (from onnxruntime->gliner)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->gliner) (25.9.23)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->gliner) (5.29.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->gliner) (1.3.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->gliner) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.4->gliner) (2025.11.12)\nDownloading gliner-0.2.24-py3-none-any.whl (151 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.9/151.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, transformers, gliner\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.57.1\n    Uninstalling transformers-4.57.1:\n      Successfully uninstalled transformers-4.57.1\nSuccessfully installed coloredlogs-15.0.1 gliner-0.2.24 humanfriendly-10.0 onnxruntime-1.23.2 transformers-4.57.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# GLiNER Zero-shot NER Evaluation (CROSSNER â€“ TEST ONLY)\n# - Test split only\n# - Batched inference â†’ manual micro-F1\n# - Fixed random seed (reproducible)\n# ============================================================\n\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport random\nimport numpy as np\n\nfrom gliner import GLiNER\n\n\n# ============================================================\n# 0. RANDOM SEED (REPRODUCIBLE)\n# ============================================================\n\nSEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# ============================================================\n# 1. CONFIG\n# ============================================================\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTHRESHOLD = 0.5\nBATCH_SIZE = 32\n\nCROSSNER_ROOT = \"/kaggle/input/crossner/ner_data\"\nCROSSNER_SUBSETS = [\"ai\", \"literature\", \"music\", \"politics\", \"science\"]\n\nMODELS = {\n    \"GLiNER-L\": \"urchade/gliner_large-v2.1\",\n}\n\n\n# ============================================================\n# 2. BIO â†’ SPANS\n# ============================================================\n\ndef bio_to_spans(tokens, tags):\n    spans = []\n    start, label = None, None\n\n    for i, t in enumerate(tags):\n        if t.startswith(\"B-\"):\n            if label is not None:\n                spans.append((start, i - 1, label))\n            start = i\n            label = t[2:]\n        elif t.startswith(\"I-\"):\n            continue\n        else:\n            if label is not None:\n                spans.append((start, i - 1, label))\n                start, label = None, None\n\n    if label is not None:\n        spans.append((start, len(tokens) - 1, label))\n\n    return spans\n\n\n# ============================================================\n# 3. LOAD CROSSNER (TEST ONLY)\n# ============================================================\n\ndef load_crossner_test(subset):\n    path = Path(CROSSNER_ROOT) / subset / \"test.txt\"\n    data, tokens, tags = [], [], []\n\n    with open(path, encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n\n            if not line:\n                if tokens:\n                    data.append({\n                        \"tokens\": tokens,\n                        \"spans\": bio_to_spans(tokens, tags)\n                    })\n                    tokens, tags = [], []\n                continue\n\n            tok, tag = line.split()\n            tokens.append(tok)\n            tags.append(tag)\n\n    return data\n\n\n# ============================================================\n# 4. CHAR â†’ TOKEN ALIGNMENT\n# ============================================================\n\ndef token_boundaries(tokens):\n    bounds, pos = [], 0\n    for t in tokens:\n        bounds.append((pos, pos + len(t)))\n        pos += len(t) + 1\n    return bounds\n\n\ndef char_to_token(preds, boundaries):\n    out = set()\n\n    for p in preds:\n        cs, ce, label = p[\"start\"], p[\"end\"], p[\"label\"]\n        s = e = None\n\n        for i, (a, b) in enumerate(boundaries):\n            if a <= cs < b:\n                s = i\n            if a < ce <= b:\n                e = i\n\n        if s is not None and e is not None and s <= e:\n            out.add((s, e, label))\n\n    return out\n\n\n# ============================================================\n# 5. BATCH INFERENCE (STORE ALL PREDICTIONS)\n# ============================================================\n\ndef run_inference(model, dataset, all_labels, desc):\n    texts = [\" \".join(ex[\"tokens\"]) for ex in dataset]\n    boundaries = [token_boundaries(ex[\"tokens\"]) for ex in dataset]\n\n    all_preds = []\n\n    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=desc):\n        batch_texts = texts[i:i + BATCH_SIZE]\n        batch_bounds = boundaries[i:i + BATCH_SIZE]\n\n        preds_batch = model.inference(\n            batch_texts,\n            all_labels,\n            threshold=THRESHOLD,\n            flat_ner=True,\n            batch_size=BATCH_SIZE,\n        )\n\n        for preds, bounds in zip(preds_batch, batch_bounds):\n            all_preds.append(char_to_token(preds, bounds))\n\n    return all_preds\n\n\n# ============================================================\n# 6. MANUAL MICRO-F1 (EXACT MATCH)\n# ============================================================\n\ndef compute_f1(dataset, preds):\n    tp = fp = fn = 0\n\n    for ex, pred in zip(dataset, preds):\n        gold = set(ex[\"spans\"])\n\n        tp += len(gold & pred)\n        fp += len(pred - gold)\n        fn += len(gold - pred)\n\n    p = tp / (tp + fp) if tp + fp > 0 else 0\n    r = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n\n    return round(f1 * 100, 1)\n\n\n# ============================================================\n# 7. LOAD DATASETS + LABEL SETS (TEST ONLY)\n# ============================================================\n\ndatasets = {\n    sub.capitalize(): load_crossner_test(sub)\n    for sub in CROSSNER_SUBSETS\n}\n\nlabel_sets = {\n    name: sorted({l for ex in ds for _, _, l in ex[\"spans\"]})\n    for name, ds in datasets.items()\n}\n\n\n# ============================================================\n# 8. RUN ALL MODELS\n# ============================================================\n\nrows = []\n\nfor model_name, model_path in MODELS.items():\n    print(f\"\\nğŸš€ Loading {model_name}\")\n    model = GLiNER.from_pretrained(model_path).to(DEVICE)\n\n    row = {\"Model\": model_name}\n    scores = []\n\n    for dname, dset in datasets.items():\n        preds = run_inference(\n            model,\n            dset,\n            label_sets[dname],\n            desc=f\"{model_name} | {dname}\"\n        )\n\n        f1 = compute_f1(dset, preds)\n        row[dname] = f1\n        scores.append(f1)\n\n    row[\"Average\"] = round(sum(scores) / len(scores), 1)\n    rows.append(row)\n\n\n# ============================================================\n# 9. RESULT TABLE\n# ============================================================\n\ndf = pd.DataFrame(rows)\ndf = df[\n    [\"Model\", \"Ai\", \"Literature\", \"Music\", \"Politics\", \"Science\", \"Average\"]\n]\n\ndisplay(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T01:46:56.919482Z","iopub.execute_input":"2025-12-26T01:46:56.919854Z","iopub.status.idle":"2025-12-26T01:47:45.839269Z","shell.execute_reply.started":"2025-12-26T01:46:56.919825Z","shell.execute_reply":"2025-12-26T01:47:45.838655Z"}},"outputs":[{"name":"stdout","text":"\nğŸš€ Loading GLiNER-L\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76958a95187c4b9db4f3a6782559ba8b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nGLiNER-L | Ai:   0%|          | 0/14 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nGLiNER-L | Ai: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:06<00:00,  2.18it/s]\nGLiNER-L | Literature: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  2.04it/s]\nGLiNER-L | Music: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:07<00:00,  2.01it/s]\nGLiNER-L | Politics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:09<00:00,  2.19it/s]\nGLiNER-L | Science: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:09<00:00,  1.76it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      Model    Ai  Literature  Music  Politics  Science  Average\n0  GLiNER-L  52.9        59.0   71.5      65.4     61.0     62.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Ai</th>\n      <th>Literature</th>\n      <th>Music</th>\n      <th>Politics</th>\n      <th>Science</th>\n      <th>Average</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GLiNER-L</td>\n      <td>52.9</td>\n      <td>59.0</td>\n      <td>71.5</td>\n      <td>65.4</td>\n      <td>61.0</td>\n      <td>62.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4}]}