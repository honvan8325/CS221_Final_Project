{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gliner transformers datasets pandas\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from datasets.features import ClassLabel, Sequence\n",
    "from gliner import GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MANUAL_LABEL_MAPS = {\n",
    "    \"WikiANN\": {\n",
    "        0: \"o\",\n",
    "        1: \"b-person\",\n",
    "        2: \"i-person\",\n",
    "        3: \"b-organization\",\n",
    "        4: \"i-organization\",\n",
    "        5: \"b-location\",\n",
    "        6: \"i-location\",\n",
    "    },\n",
    "    \"BC5CDR\": {\n",
    "        0: \"o\",\n",
    "        1: \"b-chemical\",\n",
    "        2: \"b-disease\",\n",
    "        3: \"i-disease\",\n",
    "        4: \"i-chemical\",\n",
    "    },\n",
    "    \"BC2GM\": {0: \"o\", 1: \"b-gene\", 2: \"i-gene\"},\n",
    "    \"NCBI\": {0: \"o\", 1: \"b-disease\", 2: \"i-disease\"},\n",
    "    \"TweetNER7\": {\n",
    "        0: \"b-corporation\",\n",
    "        1: \"b-creative work\",\n",
    "        2: \"b-event\",\n",
    "        3: \"b-group\",\n",
    "        4: \"b-location\",\n",
    "        5: \"b-person\",\n",
    "        6: \"b-product\",\n",
    "        7: \"i-corporation\",\n",
    "        8: \"i-creative work\",\n",
    "        9: \"i-event\",\n",
    "        10: \"i-group\",\n",
    "        11: \"i-location\",\n",
    "        12: \"i-person\",\n",
    "        13: \"i-product\",\n",
    "        14: \"o\",\n",
    "    },\n",
    "    \"Broad-Tweet\": {\n",
    "        0: \"o\",\n",
    "        1: \"b-person\",\n",
    "        2: \"i-person\",\n",
    "        3: \"b-organization\",\n",
    "        4: \"i-organization\",\n",
    "        5: \"b-location\",\n",
    "        6: \"i-location\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31112df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_dataset(name, config=None, split=None):\n",
    "    kwargs = {\"split\": split} if split else {}\n",
    "    try:\n",
    "        return load_dataset(name, config, revision=\"refs/convert/parquet\", **kwargs) if config else \\\n",
    "               load_dataset(name, revision=\"refs/convert/parquet\", **kwargs)\n",
    "    except:\n",
    "        return load_dataset(name, config, **kwargs) if config else load_dataset(name, **kwargs)\n",
    "\n",
    "def get_label_map(hf_dataset, label_col, alias):\n",
    "    if alias in MANUAL_LABEL_MAPS:\n",
    "        return MANUAL_LABEL_MAPS[alias]\n",
    "    \n",
    "    if label_col in hf_dataset.features:\n",
    "        feature = hf_dataset.features[label_col]\n",
    "        if isinstance(feature, Sequence) and isinstance(feature.feature, ClassLabel):\n",
    "            return {i: name.lower() for i, name in enumerate(feature.feature.names)}\n",
    "        if isinstance(feature, ClassLabel):\n",
    "            return {i: name.lower() for i, name in enumerate(feature.names)}\n",
    "    return None\n",
    "\n",
    "def convert_bio_to_gliner(hf_dataset, label_col, alias):\n",
    "    id2label = get_label_map(hf_dataset, label_col, alias)\n",
    "    if id2label is None:\n",
    "        print(f\"‚ö†Ô∏è Warning: No label map found for {alias}\")\n",
    "        return []\n",
    "\n",
    "    gliner_data = []\n",
    "    for ex in hf_dataset:\n",
    "        tokens = ex[\"tokens\"]\n",
    "        tags = ex[label_col]\n",
    "        if not tokens:\n",
    "            continue\n",
    "\n",
    "        spans, start, cur_type = [], None, None\n",
    "        for i, tag_id in enumerate(tags):\n",
    "            tag_str = id2label.get(tag_id, \"o\").lower()\n",
    "            \n",
    "            if tag_str == \"o\":\n",
    "                if start is not None:\n",
    "                    spans.append([start, i - 1, cur_type])\n",
    "                    start, cur_type = None, None\n",
    "                continue\n",
    "\n",
    "            prefix, ent_type = tag_str.split(\"-\", 1) if \"-\" in tag_str else (\"o\", None)\n",
    "\n",
    "            if prefix == \"s\":\n",
    "                if start is not None:\n",
    "                    spans.append([start, i - 1, cur_type])\n",
    "                    start, cur_type = None, None\n",
    "                spans.append([i, i, ent_type])\n",
    "            elif prefix == \"b\":\n",
    "                if start is not None:\n",
    "                    spans.append([start, i - 1, cur_type])\n",
    "                start, cur_type = i, ent_type\n",
    "            elif prefix == \"i\":\n",
    "                if start is None or cur_type != ent_type:\n",
    "                    if start is not None:\n",
    "                        spans.append([start, i - 1, cur_type])\n",
    "                    start, cur_type = i, ent_type\n",
    "            elif prefix == \"e\":\n",
    "                if start is not None and cur_type == ent_type:\n",
    "                    spans.append([start, i, cur_type])\n",
    "                else:\n",
    "                    spans.append([i, i, ent_type])\n",
    "                start, cur_type = None, None\n",
    "\n",
    "        if start is not None:\n",
    "            spans.append([start, len(tags) - 1, cur_type])\n",
    "\n",
    "        gliner_data.append({\"tokenized_text\": tokens, \"ner\": spans})\n",
    "    return gliner_data\n",
    "\n",
    "def convert_genia_to_gliner(hf_dataset):\n",
    "    gliner_data = []\n",
    "    for ex in hf_dataset:\n",
    "        spans = [[ent[\"start\"], ent[\"end\"] - 1, ent[\"type\"].lower()] for ent in ex[\"entities\"]]\n",
    "        gliner_data.append({\"tokenized_text\": ex[\"tokens\"], \"ner\": spans})\n",
    "    return gliner_data\n",
    "\n",
    "def load_harveyner():\n",
    "    data_dir = Path(\"harveyner\")\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    path = data_dir / \"tweets.test.bio\"\n",
    "    \n",
    "    if not path.exists():\n",
    "        try:\n",
    "            url = \"https://raw.githubusercontent.com/brickee/HarveyNER/main/data/tweets/tweets.test.bio\"\n",
    "            path.write_text(requests.get(url).text, encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to download HarveyNER: {e}\")\n",
    "            return []\n",
    "    \n",
    "    gliner_data, tokens, tags = [], [], []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not line.strip():\n",
    "            if tokens:\n",
    "                spans, start, cur_type = [], None, None\n",
    "                for i, tag in enumerate(tags):\n",
    "                    if tag == \"O\":\n",
    "                        if start is not None:\n",
    "                            spans.append([start, i - 1, cur_type])\n",
    "                            start, cur_type = None, None\n",
    "                    elif tag.startswith(\"B-\"):\n",
    "                        if start is not None:\n",
    "                            spans.append([start, i - 1, cur_type])\n",
    "                        start, cur_type = i, tag[2:].lower()\n",
    "                    elif tag.startswith(\"I-\"):\n",
    "                        ent_type = tag[2:].lower()\n",
    "                        if cur_type != ent_type:\n",
    "                            if start is not None:\n",
    "                                spans.append([start, i - 1, cur_type])\n",
    "                            start, cur_type = i, ent_type\n",
    "                \n",
    "                if start is not None:\n",
    "                    spans.append([start, len(tags) - 1, cur_type])\n",
    "                gliner_data.append({\"tokenized_text\": tokens, \"ner\": spans})\n",
    "                tokens, tags = [], []\n",
    "        else:\n",
    "            parts = line.split()\n",
    "            tokens.append(parts[0])\n",
    "            tags.append(parts[-1])\n",
    "    return gliner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_CONFIG = [\n",
    "    {\"name\": \"tner/conll2003\", \"col\": \"tags\", \"alias\": \"CoNLL03\"},\n",
    "    {\"name\": \"wikiann\", \"config\": \"en\", \"col\": \"ner_tags\", \"alias\": \"WikiANN\"},\n",
    "    {\"name\": \"ncbi_disease\", \"col\": \"ner_tags\", \"alias\": \"NCBI\"},\n",
    "    {\"name\": \"spyysalo/bc2gm_corpus\", \"col\": \"ner_tags\", \"alias\": \"BC2GM\"},\n",
    "    {\"name\": \"tner/bc5cdr\", \"col\": \"tags\", \"alias\": \"BC5CDR\"},\n",
    "    {\"name\": \"Aunderline/genia\", \"alias\": \"GENIA\"},\n",
    "    {\"name\": \"tner/tweetner7\", \"col\": \"tags\", \"alias\": \"TweetNER7\"},\n",
    "    {\"name\": \"GateNLP/broad_twitter_corpus\", \"col\": \"ner_tags\", \"alias\": \"Broad-Tweet\"},\n",
    "    {\"name\": \"Babelscape/multinerd\", \"config\": \"default\", \"col\": \"ner_tags\", \"alias\": \"MultiNERD\", \"filter_lang\": \"en\"},\n",
    "    {\"name\": \"DFKI-SLT/fabner\", \"config\": \"fabner\", \"col\": \"ner_tags\", \"alias\": \"Fabner\"},\n",
    "]\n",
    "\n",
    "test_sets = {}\n",
    "print(\"‚è≥ Loading test datasets...\\n\")\n",
    "\n",
    "for cfg in DATASETS_CONFIG:\n",
    "    print(f\"üìÇ {cfg['alias']}...\", end=\" \")\n",
    "    try:\n",
    "        ds_test = load_hf_dataset(cfg[\"name\"], cfg.get(\"config\"), split=\"test\")\n",
    "        \n",
    "        if \"filter_lang\" in cfg:\n",
    "            ds_test = ds_test.filter(lambda x: x[\"lang\"] == cfg[\"filter_lang\"])\n",
    "        \n",
    "        converted = convert_genia_to_gliner(ds_test) if cfg[\"alias\"] == \"GENIA\" else \\\n",
    "                    convert_bio_to_gliner(ds_test, cfg[\"col\"], cfg[\"alias\"])\n",
    "        \n",
    "        converted = [x for x in converted if x['ner']]\n",
    "        \n",
    "        if len(converted) > 10000:\n",
    "            converted = random.sample(converted, 10000)\n",
    "        \n",
    "        test_sets[cfg['alias']] = converted\n",
    "        print(f\"‚úÖ {len(converted)} samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {e}\")\n",
    "\n",
    "harvey = load_harveyner()\n",
    "if harvey:\n",
    "    test_sets[\"HarveyNER\"] = harvey\n",
    "    print(f\"üìÇ HarveyNER... ‚úÖ {len(harvey)} samples\")\n",
    "\n",
    "print(f\"\\n‚ú® Loaded {len(test_sets)} datasets successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"urchade/gliner_large-v2.1\"\n",
    "print(f\"üöÄ Loading GLiNER model: {MODEL_NAME}...\\n\")\n",
    "\n",
    "model = GLiNER.from_pretrained(MODEL_NAME, local_files_only=False, _attn_implementation=\"eager\").to(device)\n",
    "\n",
    "def get_token_boundaries(tokens):\n",
    "  \n",
    "    boundaries = []\n",
    "    pos = 0\n",
    "    for token in tokens:\n",
    "        boundaries.append((pos, pos + len(token)))\n",
    "        pos += len(token) + 1\n",
    "    return boundaries\n",
    "\n",
    "def align_spans_to_tokens(pred_spans, token_boundaries):\n",
    "  \n",
    "    token_spans = set()\n",
    "    \n",
    "    for pred in pred_spans:\n",
    "        char_start, char_end = pred['start'], pred['end']\n",
    "        label = pred['label'].lower()\n",
    "        \n",
    "        start_idx = end_idx = -1\n",
    "        \n",
    "        for idx, (t_start, t_end) in enumerate(token_boundaries):\n",
    "            if t_start <= char_start < t_end:\n",
    "                start_idx = idx\n",
    "                break\n",
    "            if char_start == t_end and idx + 1 < len(token_boundaries):\n",
    "                start_idx = idx + 1\n",
    "                break\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            for idx in range(start_idx, len(token_boundaries)):\n",
    "                t_start, t_end = token_boundaries[idx]\n",
    "                if t_start < char_end <= t_end:\n",
    "                    end_idx = idx\n",
    "                    break\n",
    "        \n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            token_spans.add((start_idx, end_idx, label))\n",
    "    \n",
    "    return token_spans\n",
    "\n",
    "def evaluate_model(model, test_sets):\n",
    "    results = []\n",
    "    print(f\"{'Dataset':<20} {'F1 Score':<12} Labels\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, dataset in test_sets.items():\n",
    "        if not dataset:\n",
    "            continue\n",
    "        \n",
    "        labels = list(set(label for ex in dataset for _, _, label in ex['ner']))\n",
    "        if not labels:\n",
    "            continue\n",
    "\n",
    "        tp = fp = fn = 0\n",
    "        \n",
    "        for ex in dataset:\n",
    "            tokens = ex['tokenized_text']\n",
    "            gold_spans = set((s, e, l) for s, e, l in ex['ner'])\n",
    "            text = \" \".join(tokens)\n",
    "            \n",
    "            preds = model.predict_entities(text, labels=labels, flat_ner=True, threshold=0.5)\n",
    "            boundaries = get_token_boundaries(tokens)\n",
    "            pred_spans = align_spans_to_tokens(preds, boundaries)\n",
    "            \n",
    "            tp += len(gold_spans & pred_spans)\n",
    "            fp += len(pred_spans - gold_spans)\n",
    "            fn += len(gold_spans - pred_spans)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results.append([name, round(f1 * 100, 2)])\n",
    "        print(f\"{name:<20} {f1*100:>6.2f}%       {len(labels)} types\")\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\"Dataset\", \"F1 Score\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df_results = evaluate_model(model, test_sets)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_results.to_string(index=False))\n",
    "print(f\"\\nAverage F1: {df_results['F1 Score'].mean():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
